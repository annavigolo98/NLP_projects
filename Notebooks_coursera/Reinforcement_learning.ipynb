{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aebe5cd",
   "metadata": {},
   "source": [
    "Fine tune the FLAN T5 model with reinforcement learning (PPO) and PEFT to generate less toxic summaries.\n",
    "The reward model is a binary classifier which predicts 'hate' or 'not hate'. You will use PPO (Proximal Policy  Optimization) to fine tune and reduce the toxicity of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0796f07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, GenerationConfig\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, TaskType\n",
    "\n",
    "#AutoModelForSequenceClassification is used for the reward model and toxicity evaluation\n",
    "\n",
    "\n",
    "#trl = transformer reinforcement learning lybrary\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead\n",
    "from trl import create_reference_model\n",
    "from trl.core import LengthSampler \n",
    "\n",
    "import torch \n",
    "import evaluate \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from tqdm import tqdm \n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc47f5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'google/flan-t5-base'\n",
    "hugging_face_dataset_name = 'knkarthick/dialogsum'\n",
    "dataset_original = load_dataset(hugging_face_dataset_name)\n",
    "dataset_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aaf4acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c724736b9d6846ea8204f1f3f3b735a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f17c70ec9846f69e3f4e5d7baec811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10016 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'query'],\n",
      "        num_rows: 8012\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'query'],\n",
      "        num_rows: 2004\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#DATASET PREPROCESSING\n",
    "\n",
    "#Take only a part of the dataset, with dialogues of a particular length. Wrap the dialogue in each prompt\n",
    "# and tokenize it. Save token ids in the field 'input_ids' and decoded version of the prompts in the \n",
    "# field 'query'.\n",
    "\n",
    "def build_dataset(model_name,\n",
    "                dataset_name,\n",
    "                input_min_text_length,\n",
    "                input_max_text_length):\n",
    "\n",
    "    #Use only the train part of the dataset\n",
    "    dataset = load_dataset(dataset_name, split='train')\n",
    "    #Filter the dialogues between min_length and max_length\n",
    "    dataset = dataset.filter(lambda x: len(x['dialogue']) > input_min_text_length and len(x['dialogue']) < input_max_text_length)\n",
    "\n",
    "    #prepare the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, device_map='auto')\n",
    "    #Setting devoce_map = 'auto' allows to automatically switch between CPU and GPU\n",
    "\n",
    "    def tokenize(sample):\n",
    "        prompt = f\"\"\"\n",
    "        Summarize the following conversation.\n",
    "        \n",
    "        {sample['dialogue']}\n",
    "        \n",
    "        Summary:\"\"\"\n",
    "\n",
    "        sample['input_ids'] = tokenizer.encode(prompt)\n",
    "        #This is a query requirement for the PPO library\n",
    "        sample['query'] = tokenizer.decode(sample['input_ids'])\n",
    "        return sample\n",
    "\n",
    "    #Tokenize each dialogue \n",
    "    dataset = dataset.map(tokenize, batched=False)\n",
    "    dataset.set_format(type = 'torch')\n",
    "\n",
    "    #Split the dataset into training and test \n",
    "    dataset_splits = dataset.train_test_split(test_size=0.2, shuffle=False, seed=42)\n",
    "\n",
    "    return dataset_splits\n",
    "\n",
    "dataset = build_dataset(model_name=model_name,\n",
    "                        dataset_name = hugging_face_dataset_name,\n",
    "                        input_min_text_length = 200,\n",
    "                        input_max_text_length = 1000)\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "303cadd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3492281895.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31maws s3 cp --recursive s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/ ./peft-dialogue-summary-checkpoint-from-s3/\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Here we pull the PEFT model of the 2nd LAB, but the checkpoint trained on the full dataset, not only a part of it\n",
    "aws s3 cp --recursive s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/ ./peft-dialogue-summary-checkpoint-from-s3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "572af403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formato del parametro non corretto - \"peft-dialogue-summary-checkpoint-from-s3\".\n"
     ]
    }
   ],
   "source": [
    "ls -alh ./peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "647ba0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f'Trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of model parameters: {100.0*trainable_model_params/all_model_params}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacd0ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the adapter to the original FLAN T5 model. In the previous lab we were adding the fully adapter only \n",
    "#for inferences, so there was no need for LoRA configs to do that. Now you need to pass the configs to the \n",
    "#constructed PEFT model, also putting the is_trainable to True.\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=['q','v'],\n",
    "    lora_dropout = 0.05,\n",
    "    bias='none',\n",
    "    task_type = TaskType.SEQ_2_SEQ_LM # FLAN T5\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(model,\n",
    "                                        './peft-dialogue-summary-checkpoint-from-s3/',\n",
    "                                        lora_config=lora_config,\n",
    "                                        torch_dtype=torch.float16,\n",
    "                                        device_map='auto',\n",
    "                                        is_trainable=True)\n",
    "\n",
    "#Note that only 1.4 % of the model's original parameters is trainable\n",
    "print(f'PEFT model parameters to be updated: \\n {print_number_of_trainable_parameters(peft_model)} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ae815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the PPO model passing the instruct-fine-tuned-PEFT model to it.\n",
    "#PPO will be used to optimize the RL policy against the reward model.\n",
    "ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(peft_model,\n",
    "                                                                torch_dtype=torch.bfloat16,\n",
    "                                                                is_trainable=True)\n",
    "\n",
    "#During PPO only a few parameters are updated. Specifically the parameters of the ValueHead. The number of \n",
    "#trainable parameters can be computed as (n+1)*m where n is the number of input units (here 768) and m is \n",
    "#the number of output units (you have m=1). The +1 term accounts for the bias term.\n",
    "\n",
    "print(f'PPO model parameters to be updated (ValueHead  769 params): \\n {print_number_of_trainable_parameters(ppo_model)}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddb9adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now create a copy of the PPO which will not be fine tuned (reference model). The reference model represents\n",
    "#the LLM before detoxification. None of the parameters of the PPO reference model will be updated.\n",
    "ref_model = create_reference_model(ppo_model)\n",
    "print(f'Reference model parameters to be updated: \\n {print_number_of_trainable_parameters(ref_model)}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29351593",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reinforcement Learning is a part of Machine Learning, in which agents take actions in an environment \n",
    "#aimed at maximizing their cumulative rewards. The agent's behavior is defined by the policy.\n",
    "#The goal of reinforcement learning is to learn an optimal or nearly-optimal policy that maximizes \n",
    "# the reward function.\n",
    "\n",
    "# In the previous section the original policy is based on the instruct PEFT model. This is the LLM before detoxification.\n",
    "# Now you need to define the reward model encouraging the agent to detoxify the dialogue summaries. \n",
    "# The intuitive approach would be to do some form of sentiment analysis across two classes (hate-nothate)\n",
    "# and give a higher reward if there is a chanche of getting class nothate as output.\n",
    "# Use facebook ROBERTA based hate speech model for REWARD MODEL. This model will output logits and then \n",
    "# predict probabilities across two classes: HATE or NOTHATE. The logits of the output NOTHATE will be taken as \n",
    "# a positive reward. Then the model will be fine-tuned with PPO using those reward values. \n",
    "# Create the reqired model class for the ROBERTA model. You will also need to create the corresponding tokenizer.\n",
    "# hothate = 1 hate = 0 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a487c43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_model_name = 'facebook/roberta-hate-speech-dynabench-r4-target'\n",
    "toxicity_tokenizer = AutoTokenizer.from_pretrained(toxicity_model_name, device_map='auto')\n",
    "toxicity_model = AutoModelForSequenceClassification.from_pretrained(toxicity_model_name, device_map='auto')\n",
    "print(toxicity_model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a7f894",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_toxic_context = 'I want to kiss you.'\n",
    "\n",
    "toxicity_input_ids = toxicity_tokenizer(non_toxic_context, return_tensors='pt').input_ids\n",
    "\n",
    "logits = toxicity_model(input_ids=toxicity_input_ids).logits\n",
    "print(f'logits [nothate, hate]' {logits.tolist()[0]})\n",
    "\n",
    "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
    "print(f'probabilities [nothate, hate]: {probabilities}')\n",
    "\n",
    "not_hate_index = 0\n",
    "nothate_reward = (logits[:, not_hate_index]).to_list()\n",
    "print(f'Reward (high): {nothate_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b42b60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE hugging face pipeline to simplify the usage of the reward model\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "sentiment_pipe = pipeline('sentiment-analysis',\n",
    "                            model = toxicity_model_name,\n",
    "                            device=device)\n",
    "\n",
    "reward_logits_kwargs = {\n",
    "    'top_k': None,\n",
    "    'function_to_apply': 'none',\n",
    "    'batch_size': 16\n",
    "}\n",
    "\n",
    "reward_probabilities_kwargs = {\n",
    "    'top_k': None,\n",
    "    'function_to_apply': 'softmax',\n",
    "    'batch_size': 16\n",
    "}\n",
    "\n",
    "print('Reward model output for non toxic context:')\n",
    "print(sentiment_pipe(non_toxic_context, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(non_toxic_context, **reward_probabilities_kwargs))\n",
    "\n",
    "#CAN ALSO TRY WITH TOXIC TEXTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571b0222",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATE TOXICITY\n",
    "#To evaluate the model before and after fine-funing/detoxification you need to set up a metric. The toxicity \n",
    "#score is a value between 0 and 1 where 1 indicates the maximum toxicity.\n",
    "\n",
    "toxicity_evaluator = evaluate.load('toxicity',\n",
    "                                    toxicity_model_name,\n",
    "                                    module_type='measurement',\n",
    "                                    toxic_label='hate')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a32a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_score = toxicity_evaluator.compute(predictions=[non_toxic_context])\n",
    "\n",
    "print('Toxicity score: ')\n",
    "print(toxicity_score['toxicity'])\n",
    "\n",
    "#ALSO FOR NEGATIVE INPUT SENTENCE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425ae229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This evaluator can be used to compute the toxicity of the dialogues prepared in the forst part of the lab.\n",
    "# You will need to pass the test dataset (dataset['test']), the same tokenizer, the frozen PEFT model and the \n",
    "# toxicity evaluator. Wrap all of this in the evaluate toxicity function\n",
    "\n",
    "def evaluate_toxicity(model,\n",
    "                    toxicity_evaluator,\n",
    "                    tokenizer,\n",
    "                    dataset,\n",
    "                    num_samples):\n",
    "\n",
    "    max_new_tokens = 100\n",
    "\n",
    "    toxicities = []\n",
    "    input_texts = []\n",
    "\n",
    "    for i, sample in tqdm(enumerate(dataset)):\n",
    "        input_text = sample['query']\n",
    "\n",
    "        if i > num_samples:\n",
    "            break\n",
    "\n",
    "        input_ids = tokenizer(input_text, return_tensors='pt', padding=True).input_ids\n",
    "\n",
    "        generation_config = GenerationConfig(max_new_tokens=max_new_tokens,\n",
    "                                            top_k=0.0,\n",
    "                                            top_p=1.0,\n",
    "                                            do_sample=True)\n",
    "\n",
    "        response_token_ids = model.generate(input_ids=input_ids, generation_config=generation_config)\n",
    "\n",
    "        generated_text = tokenizer.decode(response_token_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        toxicity_score = toxicity_evaluator.compute(predictions = [(input_text + '' + generated_text)])\n",
    "        toxicities.append(toxicity_score['toxicity'])\n",
    "\n",
    "    mean = np.mean(toxicities)\n",
    "    std = np.std(toxicities)\n",
    "\n",
    "    return mean, std "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b094933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform the calculation of toxicity before fine-tuning / detoxification\n",
    "\n",
    "mean_before_detoxification, std_before_detoxification = evaluate_toxicity(\n",
    "                                                                            model=ref_model,\n",
    "                                                                            toxicity_evaluator=toxicity_evaluator,\n",
    "                                                                            tokenizer=tokenizer,\n",
    "                                                                            dataset=dataset['test'],\n",
    "                                                                            num_samples=10)\n",
    "\n",
    "print(f'Toxicity [mean, std] before DETOX: [{mean_before_detoxification}, {std_before_detoxification}]' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae00f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE PPO Trainer \n",
    "# Set up the configuration parameters. Load the PPO model and the tokenizer. You will also load a frozen version \n",
    "# of the model: ref-model. The first model is optimized while the second one is frozen and serves as a \n",
    "# reference for the KL divergence term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bc171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1.41e-5\n",
    "max_ppo_epochs = 1\n",
    "mini_batch_size = 4\n",
    "batch_size = 16 \n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name = model_name,\n",
    "    learning_rate = learning_rate,\n",
    "    ppo_epochs = max_ppo_epochs,\n",
    "    mini_batch_size=mini_batch_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "ppo_trainer = PPOTrainer(config = config, \n",
    "                        model = ppo_model,\n",
    "                        ref_model = ref_model,\n",
    "                        tokenizer = tokenizer,\n",
    "                        dataset = dataset['train'],\n",
    "                        data_collator = collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5c8284",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINE TUNE THE MODEL \n",
    "#fine tuning loop consists in the following steps \n",
    "# 1) Get the query responses from the policy LLM (PEFT model)\n",
    "# 2) Get sentiments for query/responses from hate speech roberta\n",
    "# 3) Optimize policy with PPO using (query, response, reward) triplets\n",
    "\n",
    "# You will see the following metrics running:\n",
    "# objective KL: minimize kl divergence \n",
    "# ppo/returns/mean maximize mean returns \n",
    "# ppo/policy/advantages_mean maximize advantages \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eab655",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_min_length = 100\n",
    "output_max_length = 400\n",
    "\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "generation_kwargs = {\n",
    "    'min_length': 5,\n",
    "    'top_k': 0.0,\n",
    "    'top_p': 1.0,\n",
    "    'do_sample': True\n",
    "}\n",
    "\n",
    "\n",
    "reward_kwargs = {\n",
    "    'top_k': None,\n",
    "    'function_to_apply': 'none',\n",
    "    'batch_size': 16\n",
    "}\n",
    "\n",
    "\n",
    "max_ppo_steps =10\n",
    "\n",
    "for step, batch in tqdm(enumerate(ppo_trainer.data_loader)):\n",
    "    if step >= max_ppo_steps:\n",
    "        break\n",
    "    \n",
    "    prompt_tensors = batch['input_ids']\n",
    "    #GET RESPONSE FROM FLAN T5 / PEFT LLM\n",
    "    summary_tensors = []\n",
    "\n",
    "    for prompt_tensor in prompt_tensors:\n",
    "        max_new_tokens = output_length_sampler()\n",
    "        generation_kwargs['max_new_tokens'] = max_new_tokens\n",
    "        summary = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n",
    "\n",
    "        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n",
    "\n",
    "    #This is needed to be called response\n",
    "    batch['response'] = [tokenizer.decode(r.squeeze()) for r in summary_tensors]\n",
    "\n",
    "    #Compute reward outputs \n",
    "    query_response_pairs = [q + r for q,r in zip(batch['query'], batch['response'])]\n",
    "\n",
    "    rewards = sentiment_pipe(query_response_pairs, **reward_kwargs)\n",
    "\n",
    "    #Use nothate item because this is the score for the positive nothate class \n",
    "\n",
    "    reward_tensors = [torch.tensor(reward[not_hate_index]['score']) for reward in rewards]\n",
    "\n",
    "    #run PPO step \n",
    "\n",
    "    stats = ppo_trainer.step(prompt_tensors, summary_tensors, reward_tensors)\n",
    "\n",
    "    ppo_trainer.log_stats(stats, batch, reward_tensors)\n",
    "\n",
    "    print(f'objective KL': {stats['objective/kl']})\n",
    "    print(f'ppo/returns mean': {stats['ppo/returns/mean']})\n",
    "    print(f'ppo/policy/advantage mean': {stats['ppo/policy/advantages_mean']})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b14508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare original LLM and detoxified LLM with toxicity score \n",
    "\n",
    "batch_size=20\n",
    "compare_results = {}\n",
    "\n",
    "df_batch = dataset['test'][0:batch_size]\n",
    "\n",
    "compare_results['query'] = df_batch['query']\n",
    "prompt_tensors = df_batch['input_ids']\n",
    "\n",
    "summary_tensors_ref = []\n",
    "summary_tensors = []\n",
    "\n",
    "for i in tqdm(range(batch_size)):\n",
    "    gen_len = output_length_sampler()\n",
    "\n",
    "    generation_kwargs['max_new_tokens'] = gen_len\n",
    "\n",
    "    summary = ref_model.generate(\n",
    "        input_ids = torch.as_tensor(prompt_tensors[1]).unsqueeze(dim=0).to(device),\n",
    "        **generation_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "\n",
    "    summary_tensors_ref.append(summary)\n",
    "\n",
    "\n",
    "    summary = ppo_model.generate(\n",
    "        input_ids = torch.as_tensor(prompt_tensors[1]).unsqueeze(dim=0).to(device),\n",
    "        **generation_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "\n",
    "    summary_tensors.append(summary)\n",
    "\n",
    "#Decoded responses\n",
    "\n",
    "compare_results['response_before'] = [tokenizer.decode(summary_tensors_ref[i]) for i in range(batch_size)]\n",
    "compare_results['response_after'] = [tokenizer.decode(summary_tensors[i]) for i in range(batch_size)]\n",
    "\n",
    "# Sentiment analysis \n",
    "texts_before = [d + s for d, s in zip(compare_results['query'], compare_results['response_before'])]\n",
    "rewards_before = sentiment_pipe(texts_before, **reward_kwargs)\n",
    "compare_results['reward_before'] = [reward[not_hate_index]['score'] for reward in rewards_before]\n",
    "\n",
    "texts_after = [d + s for d, s in zip(compare_results['query'], compare_results['response_after'])]\n",
    "rewards_after = sentiment_pipe(texts_after, **reward_kwargs)\n",
    "compare_results['reward_after'] = [reward[not_hate_index]['score'] for reward in rewards_after]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
